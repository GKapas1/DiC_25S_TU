{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef23166-40bc-4822-8d34-c0ecd6a31fe4",
   "metadata": {},
   "source": [
    "# 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3c0f25-9719-4efa-92f9-389199dcf15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "25/05/13 19:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n",
      "25/05/13 19:06:08 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "✅ Spark session initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 46110)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import required packages and initialize Spark\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Part1_ChiSquare\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Set up logging for better visibility of stages\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(\"DIC25_Part1\")\n",
    "\n",
    "print(\"✅ Spark session initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff2247a3-7a43-47e8-bd5e-5cc0de5a023a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Current working directory: /home/e12433762/DIC2025_Ex2/src\n",
      "📂 Files in directory:\n",
      "   .ipynb_checkpoints\n",
      "   Assignment_2_pt1.ipynb\n",
      "   output\n"
     ]
    }
   ],
   "source": [
    "# Define all required paths\n",
    "REVIEWS_DEVSET_PATH = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "STOPWORDS_PATH = \"../stopwords.txt\"\n",
    "OUTPUT_PATH = \"output_rdd.txt\"\n",
    "\n",
    "# Verify current working directory and file presence\n",
    "import os\n",
    "print(\"📁 Current working directory:\", os.getcwd())\n",
    "print(\"📂 Files in directory:\")\n",
    "for fn in sorted(os.listdir(\".\")):\n",
    "    print(\"  \", fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d989c9-078f-4086-ab66-75d3bb029644",
   "metadata": {},
   "source": [
    "## Load Data and Stopwords\n",
    "\n",
    "Load the Amazon reviews development set from HDFS and stopword list from a local file. Only reviews with both category and reviewText fields are kept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e07082-16e6-47d1-81b5-14b6635c3a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DIC25_Part1:📌 Loaded 591 stopwords.\n"
     ]
    }
   ],
   "source": [
    "# Load stopwords into a Python set\n",
    "def load_stopwords(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return set(w.strip() for w in f if w.strip())\n",
    "\n",
    "stopwords = load_stopwords(STOPWORDS_PATH)\n",
    "log.info(\"📌 Loaded %d stopwords.\", len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7efe43-894d-4afe-bd17-18e9c463fee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DIC25_Part1:📈 Loaded 78829 reviews in 18.10 seconds.                      \n"
     ]
    }
   ],
   "source": [
    "# Load the reviews as an RDD of (category, reviewText)\n",
    "def get_reviews_rdd(path: str):\n",
    "    df = (\n",
    "        spark.read\n",
    "             .json(path)\n",
    "             .select(\"category\", \"reviewText\")\n",
    "             .na.drop(subset=[\"category\", \"reviewText\"])\n",
    "    )\n",
    "    return df.rdd.map(lambda row: (row[\"category\"], row[\"reviewText\"]))\n",
    "\n",
    "# Timer: Load reviews and cache for reuse\n",
    "start = time.time()\n",
    "reviews = get_reviews_rdd(REVIEWS_DEVSET_PATH).cache()\n",
    "count = reviews.count()\n",
    "log.info(\"📈 Loaded %d reviews in %.2f seconds.\", count, time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f7025-eaeb-445d-b50d-a9deab854be8",
   "metadata": {},
   "source": [
    "# 2.Preprocessing Functions & RDD Creation\n",
    "\n",
    "## Preprocessing and Tokenization\n",
    "\n",
    "Reviews are tokenized using the specified delimiters, lowercased, and filtered for stopwords and one-character terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b0d1c4-f570-4122-9a4e-1c3fbc4d2b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DIC25_Part1:🧹 Preprocessed 78829 records in 3.19 seconds.                 \n",
      "INFO:DIC25_Part1:📖 Category: Patio_Lawn_and_Garde → 31 tokens\n",
      "INFO:DIC25_Part1:📖 Category: Patio_Lawn_and_Garde → 34 tokens\n",
      "INFO:DIC25_Part1:📖 Category: Patio_Lawn_and_Garde → 32 tokens\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: tokenization, lowercasing, stopword removal, length filtering\n",
    "import re\n",
    "\n",
    "DELIMITERS = (\n",
    "    r'[\\s\\t\\d\\(\\)\\[\\]\\{\\}\\.!?,;:+=\\-_\"]|\\'|`|~|#|@|&|%|\\*|\\\\/|\\u20AC|\\$|\\u00A7'\n",
    ")\n",
    "token_split_re = re.compile(DELIMITERS)\n",
    "\n",
    "def preprocess_record(record):\n",
    "    category, text = record\n",
    "    tokens = token_split_re.split(text.lower())\n",
    "    filtered = [t for t in tokens if t and t not in stopwords and len(t) > 1]\n",
    "    return category, filtered\n",
    "\n",
    "# Apply preprocessing and cache result\n",
    "start = time.time()\n",
    "cat_tokens = reviews.map(preprocess_record).cache()\n",
    "cat_tokens_count = cat_tokens.count()\n",
    "log.info(\"🧹 Preprocessed %d records in %.2f seconds.\", cat_tokens_count, time.time() - start)\n",
    "\n",
    "# Quick sample to verify\n",
    "for cat, toks in cat_tokens.take(3):\n",
    "    log.info(\"📖 Category: %s → %d tokens\", cat, len(toks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef223b49-6403-4542-a601-c722094a58d5",
   "metadata": {},
   "source": [
    "# 3. Token‐Category Counts with Spark\n",
    "\n",
    "## Step: Compute A (term present in category)\n",
    "Emit ((term, category), 1) once per document where term occurs. This gives A: documents in category where the term appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38a6ff7b-fce7-4a88-9bd1-6266b3d94034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DIC25_Part1:📄 Counted document-level (token, category) pairs in 0.14 seconds.\n",
      "INFO:DIC25_Part1:Sample document-level token-category counts: [(('studio', 'CDs_and_Vinyl'), 92), (('warm', 'CDs_and_Vinyl'), 42), (('stafford', 'CDs_and_Vinyl'), 1)]\n"
     ]
    }
   ],
   "source": [
    "# Count in how many documents each token appears per category (document-level frequency)\n",
    "\n",
    "def map_record_to_doc_flags(records):\n",
    "    \"\"\"\n",
    "    For each document (category, tokens), emit ((token, category), 1)\n",
    "    only once per document. This ensures document-level statistics for chi-square.\n",
    "\n",
    "    Input: iterator of (category, tokens)\n",
    "    Output: ((token, category), 1) for each unique token in the document\n",
    "    \"\"\"\n",
    "    for category, tokens in records:\n",
    "        unique_tokens = set(tokens)  # Remove duplicates within the document\n",
    "        for token in unique_tokens:\n",
    "            yield ((token, category), 1)\n",
    "\n",
    "# Apply the transformation and reduce by key\n",
    "start = time.time()\n",
    "token_cat_doc_counts = (\n",
    "    cat_tokens.mapPartitions(map_record_to_doc_flags)\n",
    "              .reduceByKey(lambda a, b: a + b)  # Sum document counts per (token, category)\n",
    "              .cache()\n",
    ")\n",
    "\n",
    "log.info(\"📄 Counted document-level (token, category) pairs in %.2f seconds.\", time.time() - start)\n",
    "log.info(\"Sample document-level token-category counts: %s\", token_cat_doc_counts.take(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9afc9f-e221-47bb-be4a-73a0bc0abe2c",
   "metadata": {},
   "source": [
    "# 4. Count in how many documents each token appears (global document frequency)\n",
    "\n",
    "## Step: Compute A + B (term present anywhere)\n",
    "Map ((term, category), count) → (term, count) and aggregate to get total doc count per term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93628907-54e1-4808-82c4-ac5f664b9c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DIC25_Part1:📊 Computed document frequency per token in 0.08 seconds.\n",
      "INFO:DIC25_Part1:Sample token document frequencies: [('insight', 429), ('things', 3563), ('open', 1305)]\n"
     ]
    }
   ],
   "source": [
    "# Compute document frequency per token: number of documents the token appeared in (across all categories)\n",
    "\n",
    "start = time.time()\n",
    "token_doc_freqs = (\n",
    "    token_cat_doc_counts  # use document-level counts!\n",
    "    .map(lambda x: (x[0][0], x[1]))  # From ((token, category), count) to (token, doc_count)\n",
    "    .reduceByKey(lambda a, b: a + b)  # Total document count per token\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "log.info(\"📊 Computed document frequency per token in %.2f seconds.\", time.time() - start)\n",
    "log.info(\"Sample token document frequencies: %s\", token_doc_freqs.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146a816-6766-4fb8-ba5a-7c4d4e691889",
   "metadata": {},
   "source": [
    "# 5. Count number of documents in each category (category document frequency)\n",
    "\n",
    "## Step: Compute category and total document counts\n",
    "This prepares:\n",
    "\n",
    "Total docs in a category (C + A)\n",
    "\n",
    "Total number of documents (A+B+C+D)\n",
    "\n",
    "These are used to derive C and D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e323a03-7494-4c52-9455-a12dc3546ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DIC25_Part1:📁 Category document counts: [('Kindle_Store', 3205), ('Electronic', 7825), ('Movies_and_TV', 4607), ('Tools_and_Home_Improvement', 1926), ('Grocery_and_Gourmet_Food', 1297), ('Apps_for_Android', 2638), ('Book', 22507), ('Toys_and_Game', 2253), ('Office_Product', 1243), ('Digital_Music', 836), ('Automotive', 1374), ('Beauty', 2023), ('Patio_Lawn_and_Garde', 994), ('Sports_and_Outdoor', 3269), ('Musical_Instrument', 500), ('CDs_and_Vinyl', 3749), ('Clothing_Shoes_and_Jewelry', 5749), ('Home_and_Kitche', 4254), ('Cell_Phones_and_Accessorie', 3447), ('Pet_Supplie', 1235), ('Baby', 916), ('Health_and_Personal_Care', 2982)]\n",
      "INFO:DIC25_Part1:🧮 Total number of documents: 78829\n",
      "INFO:DIC25_Part1:⏱️ Category doc count done in 1.14 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Compute number of documents per category (df_c), and total document count\n",
    "\n",
    "start = time.time()\n",
    "docs_per_cat = (\n",
    "    cat_tokens\n",
    "    .map(lambda x: (x[0], 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "total_docs = docs_per_cat.map(lambda x: x[1]).sum()\n",
    "log.info(\"📁 Category document counts: %s\", docs_per_cat.collect())\n",
    "log.info(\"🧮 Total number of documents: %d\", total_docs)\n",
    "log.info(\"⏱️ Category doc count done in %.2f seconds.\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cec1a42-5ce6-4eb8-9f29-60f237c4a7d4",
   "metadata": {},
   "source": [
    "# 6. Chi-square calculation using document-level frequencies\n",
    "\n",
    "## Step: Compute Chi-square Scores from A, B, C, D\n",
    "\n",
    "This step constructs a 2×2 contingency table for each (term, category) pair and calculates the corresponding chi-square score.\n",
    "\n",
    "### The contingency table is based on:\n",
    "\n",
    "A: Count of documents containing the term and belonging to the category\n",
    "\n",
    "B: Count of documents containing the term but not belonging to the category\n",
    "\n",
    "C: Count of documents not containing the term but belonging to the category\n",
    "\n",
    "D: Count of documents neither containing the term nor belonging to the category\n",
    "\n",
    "### These values are derived as follows:\n",
    "\n",
    "A: From (term, category) document frequency\n",
    "\n",
    "B: Term total across all documents minus A\n",
    "\n",
    "C: Category total minus A\n",
    "\n",
    "D: Total documents − A − B − C\n",
    "\n",
    "The chi-square statistic is then computed for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc03b96-631f-4e25-a279-e3308a17c11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DIC25_Part1:✅ Normalized chi-square calculation done in 0.83 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample χ² top-75 terms for a few categories:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apps_for_Android → [('games', 3086.222198300114), ('play', 2166.658247313346), ('graphics', 1541.6385306131185), ('kindle', 1474.0208809190092), ('addictive', 1311.905562727777), ('challenging', 1038.1284558527927), ('coins', 1015.5899629723237), ('addicting', 990.8441134974868), ('fire', 961.4880765608062), ('levels', 828.0306568656633), ('playing', 693.7937006696752), ('ads', 651.3749934344062), ('puzzles', 583.3218622383833), ('apps', 548.7810653104153), ('free', 502.34201866039217), ('bingo', 409.2358492981346), ('mahjong', 322.00891943980963), ('download', 308.794407744245), ('faotd', 288.8577201586641), ('facebook', 282.51705437029005), ('downloaded', 262.77022492215735), ('hints', 242.61029019440056), ('android', 213.37008082383736), ('solitaire', 211.6429957838186), ('gameplay', 198.5123356770461), ('unlock', 190.27341706776008), ('freezes', 189.67737127837006), ('played', 180.78646373662968), ('deleted', 179.2243589462116), ('bought', 174.4587211734982), ('flappy', 173.30583696524425), ('puzzle', 170.36675686406016), ('upgrades', 168.99856742047183), ('tablet', 157.7010882536515), ('awesome', 155.41071644716388), ('price', 149.28818406799732), ('calculator', 148.95756302858823), ('developer', 148.37746519215403), ('quality', 143.84845750146533), ('permissions', 137.37091512558038), ('author', 134.8840986483009), ('earn', 134.7527163155313), ('uninstall', 134.6178102338016), ('bored', 131.23190073884672), ('sudoku', 126.99968483964763), ('fit', 125.99007144278971), ('years', 124.320322051071), ('reading', 121.46497244325894), ('made', 120.84128900049052), ('characters', 118.022624950193), ('gameloft', 115.53429315666729), ('series', 114.94413717913604), ('written', 114.80298631422544), ('addicted', 109.65036786251359), ('brain', 108.94504098081251), ('crashes', 107.3093981107394), ('multiplayer', 102.51457697684802), ('challenge', 102.18967874435549), ('sims', 100.44868563758436), ('uninstalling', 100.32193506574053), ('tetris', 99.24357491450239), ('glitches', 99.02553697390849), ('back', 98.74610911501135), ('end', 95.17497691720685), ('waster', 94.22529346826073), ('crashing', 93.68803122186573), ('downloading', 93.54235192211287), ('size', 93.0844967751577), ('put', 91.50633514370045), ('minecraft', 88.26571319760414), ('logos', 88.26571319760414), ('frustrating', 87.13612410083218), ('freecell', 86.64962060177764), ('apos', 86.64962060177764), ('devs', 86.64962060177764)]\n",
      "Book → [('reading', 6187.400331425424), ('author', 6161.55936616974), ('characters', 4818.2379708917415), ('written', 4297.997535424257), ('series', 2863.2719784703795), ('reader', 2784.2362035812002), ('writing', 2433.9125361817187), ('interesting', 2377.898238056555), ('world', 2160.113745891937), ('enjoyed', 2121.4678992128647), ('pages', 2111.0393841901764), ('stories', 2039.4195676396987), ('character', 1945.2520208977464), ('page', 1780.7368427686806), ('history', 1777.977623590975), ('information', 1719.1355543242184), ('readers', 1498.5282415729732), ('chapter', 1448.539068756305), ('plot', 1355.1371465764037), ('price', 1323.8579188756762), ('authors', 1302.2628807021358), ('quality', 1300.0497633261682), ('people', 1237.4427862672794), ('understand', 1128.577026656057), ('romance', 1113.4564224446156), ('chapters', 1104.2811715544833), ('writer', 1049.6495627864058), ('lives', 1038.7406252250871), ('mystery', 1028.768052495006), ('fit', 1021.7767325596747), ('novels', 1017.885664454523), ('end', 985.2112546633374), ('loved', 979.7584990063906), ('man', 926.2057500535855), ('learn', 922.9969757963519), ('god', 917.3200281421858), ('ending', 912.3281586884466), ('ideas', 897.2673762624403), ('fiction', 889.3548419866793), ('human', 877.1798688060966), ('relationship', 871.7991306475546), ('understanding', 863.3424409381867), ('subject', 844.0747631294289), ('bought', 834.3748605349716), ('war', 823.1566904956876), ('interested', 797.2308974972292), ('young', 788.0370276023139), ('finished', 786.9801226898121), ('writes', 778.5311642352611), ('size', 777.0926434247457), ('sense', 773.2875243499298), ('family', 768.676487703572), ('historical', 761.6717686590193), ('works', 752.3845112905688), ('examples', 741.9786836602917), ('questions', 739.3846570259296), ('knowledge', 735.2525733970558), ('text', 728.966971632536), ('events', 725.7619958872682), ('recipes', 725.2375889705995), ('interest', 722.3278974570063), ('found', 719.2751190487941), ('published', 718.6007351013919), ('wait', 716.3384129402905), ('illustrations', 701.0611082460483), ('children', 691.8944724135298), ('forward', 690.4719055748874), ('journey', 688.866758311885), ('study', 687.5846085296656), ('felt', 682.0694751202252), ('heart', 676.4489389513035), ('reference', 674.3483986892071), ('guide', 672.3186203889878), ('personal', 667.6303737193788), ('copy', 665.8860324320367)]\n",
      "Toys_and_Game → [('toys', 1390.1728600363515), ('loves', 1251.6935512104387), ('lego', 1219.5840386320606), ('son', 1175.8639151699658), ('dolls', 937.5559944440915), ('grandson', 935.9961034371413), ('play', 843.7353173225702), ('birthday', 828.2941687874674), ('kids', 802.1673844746103), ('christmas', 724.5103728351675), ('year', 667.4847072732169), ('daughter', 659.0934931867271), ('legos', 578.329484379666), ('puzzle', 562.0571027846358), ('pieces', 519.2522141479096), ('playing', 449.84979387580887), ('rc', 425.1210975081267), ('cards', 415.7884700099335), ('granddaughter', 370.3591501410525), ('cute', 356.9178043725965), ('hasbro', 305.93106699314217), ('barbie', 291.3459190760446), ('gift', 290.21941312763624), ('nephew', 264.5791346322608), ('child', 257.3749231016147), ('nerf', 250.25949437711128), ('played', 235.96071606784523), ('durable', 235.66453227164644), ('articulation', 227.6363780107492), ('figures', 226.42191021885762), ('helicopters', 220.05276433380646), ('cuddly', 217.52044265012037), ('dice', 217.47328442294486), ('leapfrog', 214.36510530666482), ('mattel', 203.9462821799007), ('paint', 200.31350691349513), ('niece', 196.8789133295974), ('educational', 196.63561157640967), ('stuffed', 192.63990614468864), ('plush', 183.5876460781194), ('cube', 182.85542325242287), ('playset', 173.10308233162377), ('games', 170.06663669112532), ('heli', 169.95307901428404), ('copter', 169.95307901428404), ('puzzles', 167.52718924074276), ('grand', 164.71610800856268), ('grandsons', 164.46272993755258), ('doug', 164.46272993755258), ('melissa', 158.14689502965663), ('plays', 156.3702474878366), ('yr', 154.82717010943247), ('bought', 149.7019285653684), ('plastic', 145.1825750022885), ('transformers', 144.11337430842954), ('dollhouse', 139.96747309016087), ('board', 138.18390072754255), ('lipo', 135.9607383466862), ('posable', 135.9607383466862), ('kid', 134.1372348053791), ('granddaughters', 130.52454556513163), ('decks', 130.23184352547042), ('train', 129.47155146244833), ('cars', 124.24009127444675), ('figure', 121.48476945292248), ('adorable', 121.2284106059588), ('collectible', 119.65195615659827), ('darts', 119.65195615659827), ('wheels', 118.59471605994135), ('sculpt', 118.55755400038623), ('reading', 116.33583446385597), ('wooden', 111.8276214355665), ('doh', 107.17575380065725), ('huggable', 107.17575380065725), ('crayons', 103.70756894569193)]\n",
      "Office_Product → [('cartridges', 5038.569370197361), ('cartridge', 3535.4142072016666), ('printing', 2838.285933971684), ('printers', 2162.8337434348077), ('epson', 1903.7877930810766), ('print', 1655.6731343560793), ('scanner', 1611.0239254653493), ('pen', 1139.4237506521965), ('toner', 1069.9176992860057), ('paper', 1017.9810486534648), ('prints', 932.2939090119022), ('pens', 926.1416272849003), ('hp', 883.2896544753319), ('scan', 787.1438464964777), ('scanning', 698.8696485771379), ('fax', 685.2382264921703), ('stapler', 673.6122313116412), ('inks', 673.6122313116412), ('shredder', 614.5417214625822), ('handset', 600.399198579727), ('lexmark', 561.829229179472), ('pencil', 504.4236893738807), ('desk', 501.1811311097976), ('labels', 463.3105981997472), ('remanufactured', 456.42276712449836), ('pencils', 453.4302896196644), ('staples', 453.3804466260599), ('laser', 433.07690917678264), ('office', 400.1518906265132), ('scans', 388.39070824637895), ('inkjet', 383.45878347584807), ('receipts', 382.7283718470046), ('copier', 380.6029774528741), ('pixma', 374.53856398873415), ('calculator', 346.4869366249256), ('stapling', 336.767657713155), ('answering', 334.9594211097334), ('mfc', 312.11151034037846), ('dymo', 312.11151034037846), ('swingline', 312.11151034037846), ('handsets', 301.7025712057345), ('landline', 301.7025712057345), ('envelopes', 286.92276008815696), ('ooma', 277.9187574246963), ('canon', 275.30328814993794), ('binders', 273.01583624964974), ('eraser', 258.9532056340054), ('borderless', 249.6860406324896), ('deskjet', 249.6860406324896), ('staplers', 249.6860406324896), ('printed', 242.2893977278895), ('avery', 238.10404957947404), ('papers', 229.65336520443654), ('refill', 224.94138901121005), ('sharpeners', 220.09420885289808), ('nib', 219.95957066188586), ('folders', 214.52536988825858), ('wireless', 206.769087466868), ('scanned', 201.13860987576587), ('uniden', 198.88783160532523), ('artisan', 198.88783160532523), ('fellowes', 198.1544695154825), ('printhead', 187.26215480478515), ('pentel', 187.26215480478515), ('cis', 187.26215480478515), ('speakerphone', 181.33069460517368), ('dpi', 181.33069460517368), ('binder', 175.50739299583654), ('erase', 169.8103687886152), ('sheets', 167.6887407627011), ('mousepad', 166.47709774136607), ('erasers', 163.8053957759705), ('refills', 162.207747331757), ('software', 159.73321979857133), ('phones', 157.9119511797778)]\n",
      "Digital_Music → [('music', 1425.7033086400636), ('lyrics', 892.1972547832194), ('listen', 771.797221751272), ('heard', 582.1779001413753), ('albums', 560.5394700569445), ('voice', 509.15970525837633), ('playlist', 458.6509735208863), ('vocals', 455.2539761706623), ('catchy', 437.9724389905707), ('band', 380.74937352365595), ('tracks', 349.1785981055151), ('sing', 346.05953362299556), ('upbeat', 344.78176803182185), ('listening', 321.4527364448787), ('mp', 318.84003601157457), ('singing', 301.8889619416554), ('sings', 291.77220224779563), ('rock', 276.3744834019482), ('singer', 256.80198361133444), ('sounds', 244.4991148833013), ('rap', 229.37888649257118), ('artist', 224.1185511915201), ('track', 216.09137299554416), ('radio', 211.68681370892793), ('hear', 209.67814610644118), ('jazz', 206.06480872940836), ('sung', 203.63434141698568), ('sound', 199.01047213033715), ('ballad', 190.03298317830715), ('hackett', 186.5908584682719), ('spangled', 186.5908584682719), ('nastradamus', 186.5908584682719), ('bloops', 186.5908584682719), ('bleeps', 186.5908584682719), ('crusin', 186.5908584682719), ('listened', 186.1556892761928), ('download', 178.84163234980122), ('ep', 170.61835555399765), ('tune', 166.91316685545726), ('cher', 161.45809697067972), ('drums', 156.98448070773887), ('funk', 153.76204563753134), ('talented', 145.71245801217498), ('dance', 144.29247779781448), ('artists', 142.93894755757844), ('falsetto', 136.96609672142438), ('soulful', 136.7810269147262), ('groove', 136.3917958778312), ('sounding', 135.53957030309581), ('recording', 134.73856044207292), ('blues', 129.3279799011151), ('indie', 125.42747366814855), ('withers', 123.06567275588536), ('pretenders', 123.06567275588536), ('shoegaze', 123.06567275588536), ('krall', 123.06567275588536), ('yodeling', 123.06567275588536), ('songstress', 123.06567275588536), ('pharrell', 123.06567275588536), ('guitarist', 122.85265573238563), ('musicians', 122.10590991611463), ('listens', 121.12425254219617), ('knopfler', 116.55450178576368), ('lyrically', 116.00783044057839), ('classic', 113.48362522803997), ('recorded', 113.43458544909907), ('record', 113.42350123411526), ('bands', 112.10836283929032), ('vocal', 109.95879531688345), ('released', 109.69331103735988), ('rapping', 109.26463743549478), ('bonnie', 106.52955416705034), ('chorus', 105.58410090727992), ('performer', 104.03937401691877), ('bluesy', 103.60238990987567)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 6. Chi-square calculation using broadcasted document-level counts (normalized version)\n",
    "\n",
    "def compute_chi2_top_terms(token_cat_doc_counts_rdd, \n",
    "                           category_doc_counts_rdd, \n",
    "                           token_doc_freqs_rdd, \n",
    "                           top_k=75):\n",
    "    \"\"\"\n",
    "    Computes top-K tokens by chi-square score per category using a stable and normalized formula.\n",
    "\n",
    "    Inputs:\n",
    "      token_cat_doc_counts_rdd: RDD[((token, category), A)]\n",
    "      category_doc_counts_rdd:   RDD[(category, df_c)]\n",
    "      token_doc_freqs_rdd:       RDD[(token, df_t)]\n",
    "      top_k: number of top terms per category to select\n",
    "\n",
    "    Returns:\n",
    "      RDD[(category, [(token, chi2)])] with top-k terms per category\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Collect small side-tables and broadcast\n",
    "    category_doc_map = category_doc_counts_rdd.collectAsMap()\n",
    "    token_doc_map = token_doc_freqs_rdd.collectAsMap()\n",
    "    total_docs = sum(category_doc_map.values())\n",
    "\n",
    "    bc_cat_docs = sc.broadcast(category_doc_map)\n",
    "    bc_token_docs = sc.broadcast(token_doc_map)\n",
    "\n",
    "    # 2. Compute chi-square with normalized denominator\n",
    "    def compute_chi2(kv):\n",
    "        (token, category), A = kv\n",
    "        N_i = bc_cat_docs.value.get(category, 0)     # df_c\n",
    "        n_j = bc_token_docs.value.get(token, 0)      # df_t\n",
    "        N = total_docs\n",
    "\n",
    "        B = N_i - A\n",
    "        C = n_j - A\n",
    "        D = N - A - B - C\n",
    "\n",
    "        num = (A * D - B * C) ** 2\n",
    "        den = (A + B) * (C + D) * (A + C) * (B + D)\n",
    "        chi2 = (N * num / den) if den != 0 else 0.0\n",
    "\n",
    "        return (category, (token, chi2))\n",
    "\n",
    "    chi2_rdd = token_cat_doc_counts_rdd.map(compute_chi2)\n",
    "\n",
    "    # 3. Group by category and take top-K\n",
    "    top_rdd = (\n",
    "        chi2_rdd\n",
    "        .groupByKey()\n",
    "        .mapValues(lambda seq: sorted(seq, key=lambda x: -x[1])[:top_k])\n",
    "        .cache()\n",
    "    )\n",
    "\n",
    "    return top_rdd\n",
    "\n",
    "# Run updated version\n",
    "start = time.time()\n",
    "\n",
    "chi_scores = compute_chi2_top_terms(\n",
    "    token_cat_doc_counts_rdd=token_cat_doc_counts,\n",
    "    category_doc_counts_rdd=docs_per_cat,\n",
    "    token_doc_freqs_rdd=token_doc_freqs,\n",
    "    top_k=75\n",
    ")\n",
    "\n",
    "log.info(\"✅ Normalized chi-square calculation done in %.2f seconds.\", time.time() - start)\n",
    "print(\"Sample χ² top-75 terms for a few categories:\")\n",
    "for category, term_list in chi_scores.take(5):\n",
    "    print(f\"{category} → {term_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0c441-acc0-4d31-ac8d-8ead168cf1e3",
   "metadata": {},
   "source": [
    "# 8. Merge chi-squared outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cfd8fe-91e8-4c56-9661-b0c9fc7a80a9",
   "metadata": {},
   "source": [
    "**Why we don’t need a separate “merge χ² outputs” step in this implementation**\n",
    "\n",
    "In this notebook, the true final output is written in the very next cell using pure Python:\n",
    "\n",
    "1. We call `chi2_top75.collect()` (see the “Collect and sort…” cell) to build `cat_lines_sorted`.  \n",
    "2. We then open `output/output_rdd.txt` and write each category line plus the merged dictionary in one go with a simple `open(..., \"w\")` loop.\n",
    "\n",
    "Because that Python block produces the exact `output_rdd.txt` we need, a preceding Spark-based cleanup/merging `coalesce(1).saveAsTextFile(\"output/merged_chi2_output.txt\")` would be redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778ca8e-d8ea-45eb-b4e6-a2e182f42961",
   "metadata": {},
   "source": [
    "# 9. Final Formatting: Sorting, Token Dictionary, and Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "424f1031-8fb4-4c54-9ed1-9a87dfb64489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote final formatted output to output/output_rdd.txt\n"
     ]
    }
   ],
   "source": [
    "# 1) Collect and sort the (category, [ (term, χ²), ... ]) lines\n",
    "cat_lines = chi_scores.collect()  # e.g. [(\"Books\", [(\"author\", 123.4), ...]), ...]\n",
    "\n",
    "# Convert to string format: \"term1:score1 term2:score2 ...\"\n",
    "cat_lines = [\n",
    "    (cat, \" \".join(f\"{tok}:{score:.4f}\" for tok, score in toks))\n",
    "    for cat, toks in cat_lines\n",
    "]\n",
    "\n",
    "# Sort categories alphabetically\n",
    "cat_lines_sorted = sorted(cat_lines, key=lambda kv: kv[0])\n",
    "\n",
    "# 2) Compute the set of all unique tokens from the formatted term strings\n",
    "unique_terms = {\n",
    "    term_score.split(\":\", 1)[0]\n",
    "    for _, term_list in cat_lines_sorted\n",
    "    for term_score in term_list.split()\n",
    "}\n",
    "unique_terms_sorted = sorted(unique_terms)\n",
    "\n",
    "# 3) Write the output in required format\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "with open(\"output/output_rdd.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    for category, term_list in cat_lines_sorted:\n",
    "        fout.write(f\"{category} {term_list}\\n\")\n",
    "    fout.write(\" \".join(unique_terms_sorted) + \"\\n\")\n",
    "\n",
    "print(\"✅ Wrote final formatted output to output/output_rdd.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
