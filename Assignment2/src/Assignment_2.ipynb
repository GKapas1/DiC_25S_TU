{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef23166-40bc-4822-8d34-c0ecd6a31fe4",
   "metadata": {},
   "source": [
    "# 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd3c0f25-9719-4efa-92f9-389199dcf15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports and SparkContext setup\n",
    "import json\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# If using a plain Python 3 kernel, bootstrap Spark; \n",
    "# if you’re on the “PySpark Python 3” kernel, spark/sc are already available.\n",
    "spark = SparkSession.builder.appName(\"Part1_ChiSquare\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# 2. Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(\"DIC25_Part1\")\n",
    "\n",
    "# 3. Relative paths (project root)\n",
    "REVIEWS_DEVSET_PATH   = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "STOPWORDS_PATH = \"stopwords.txt\"\n",
    "OUTPUT_PATH    = \"output_rdd.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff2247a3-7a43-47e8-bd5e-5cc0de5a023a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/e12433762/DIC2025_Ex2\n",
      "Here’s what lives in this folder:\n",
      "  .ipynb_checkpoints\n",
      "  reviews_devset.json\n",
      "  src\n",
      "  stopwords.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Here’s what lives in this folder:\")\n",
    "for fn in sorted(os.listdir(\".\")):\n",
    "    print(\" \", fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50e07082-16e6-47d1-81b5-14b6635c3a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DIC25_Part1:Loaded 591 stopwords\n"
     ]
    }
   ],
   "source": [
    "# Load the provided stopword list into a Python set\n",
    "def load_stopwords(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return set(w.strip() for w in f if w.strip())\n",
    "\n",
    "stopwords = load_stopwords(STOPWORDS_PATH)\n",
    "log.info(\"Loaded %d stopwords\", len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb7efe43-894d-4afe-bd17-18e9c463fee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DIC25_Part1:Loaded 78829 reviews\n"
     ]
    }
   ],
   "source": [
    "def get_reviews_rdd(path: str):\n",
    "    \"\"\"\n",
    "    Read a JSON-Lines file into a DataFrame,\n",
    "    select category & reviewText, drop nulls,\n",
    "    then convert to RDD[(category, text)].\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        spark.read           # ← default: each line is one JSON record\n",
    "             .json(path)     # no .option(\"multiline\",\"true\")\n",
    "             .select(\"category\", \"reviewText\")\n",
    "             .na.drop(subset=[\"category\", \"reviewText\"])\n",
    "    )\n",
    "\n",
    "    return df.rdd.map(lambda row: (row[\"category\"], row[\"reviewText\"]))\n",
    "\n",
    "reviews = get_reviews_rdd(REVIEWS_DEVSET_PATH)\n",
    "log.info(\"Loaded %d reviews\", reviews.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f7025-eaeb-445d-b50d-a9deab854be8",
   "metadata": {},
   "source": [
    "# 2.Preprocessing Functions & RDD Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75b0d1c4-f570-4122-9a4e-1c3fbc4d2b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DIC25_Part1:Preprocessed 78829 reviews\n",
      "INFO:DIC25_Part1:Patio_Lawn_and_Garde → 31 tokens\n",
      "INFO:DIC25_Part1:Patio_Lawn_and_Garde → 34 tokens\n",
      "INFO:DIC25_Part1:Patio_Lawn_and_Garde → 32 tokens\n",
      "INFO:DIC25_Part1:Patio_Lawn_and_Garde → 21 tokens\n",
      "INFO:DIC25_Part1:Patio_Lawn_and_Garde → 16 tokens\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 3.1 Define the exact delimiters from preprocessing.py\n",
    "DELIMITERS = (\n",
    "    r'[\\s\\t\\d\\(\\)\\[\\]\\{\\}\\.!\\?,;:+=\\-_\"]'\n",
    "    r\"|\\'|`|~|#|@|&|%|\\*|\\\\/|\\u20AC|\\$|\\u00A7\"\n",
    ")\n",
    "token_split_re = re.compile(DELIMITERS)\n",
    "\n",
    "# 3.2 Preprocessing function\n",
    "def preprocess_record(record):\n",
    "    \"\"\"\n",
    "    Input:  (category, reviewText)\n",
    "    Output: (category, [filtered tokens])\n",
    "    \"\"\"\n",
    "    category, text = record\n",
    "    # lowercase + split on all delimiters\n",
    "    tokens = token_split_re.split(text.lower())\n",
    "    # drop empty strings, stopwords, and tokens of length <= 1\n",
    "    filtered = [t for t in tokens if t and t not in stopwords and len(t) > 1]\n",
    "    return category, filtered\n",
    "\n",
    "# 3.3 Apply to the reviews RDD\n",
    "cat_tokens = reviews.map(preprocess_record)\n",
    "\n",
    "# 3.4 Quick check & logging\n",
    "log.info(\"Preprocessed %d reviews\", cat_tokens.count())\n",
    "for cat, toks in cat_tokens.take(5):\n",
    "    log.info(\"%s → %d tokens\", cat, len(toks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef223b49-6403-4542-a601-c722094a58d5",
   "metadata": {},
   "source": [
    "# 4. Token‐Category Counts with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38a6ff7b-fce7-4a88-9bd1-6266b3d94034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample token-category counts:\n",
      "'amp' in 'CDs_and_Vinyl' → 93\n",
      "'glad' in 'CDs_and_Vinyl' → 94\n",
      "'studio' in 'CDs_and_Vinyl' → 104\n",
      "'radio' in 'CDs_and_Vinyl' → 185\n",
      "'recordings' in 'CDs_and_Vinyl' → 207\n",
      "'section' in 'CDs_and_Vinyl' → 60\n",
      "'shaw' in 'CDs_and_Vinyl' → 4\n",
      "'cole' in 'CDs_and_Vinyl' → 22\n",
      "'gordon' in 'CDs_and_Vinyl' → 22\n",
      "'johnny' in 'CDs_and_Vinyl' → 36\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def map_record_to_counts(records):\n",
    "    \"\"\"\n",
    "    Mapper‐style local aggregation:\n",
    "      Input:  iterator over (category, tokens) pairs\n",
    "      Output: ((token, category), count) for each token in each record\n",
    "    \"\"\"\n",
    "    for category, tokens in records:\n",
    "        # local Counter to collapse duplicate tokens within this record\n",
    "        local_counts = Counter((token, category) for token in tokens)\n",
    "        for tok_cat, cnt in local_counts.items():\n",
    "            yield tok_cat, cnt\n",
    "\n",
    "# Compute ((token, category), total_count) across the corpus\n",
    "token_cat_counts = (\n",
    "    cat_tokens\n",
    "      .mapPartitions(map_record_to_counts)       # local pre‐aggregation\n",
    "      .reduceByKey(lambda a, b: a + b)          # global aggregation\n",
    ")\n",
    "\n",
    "# Inspect a sample of results\n",
    "print(\"Sample token-category counts:\")\n",
    "for (token, category), count in token_cat_counts.take(10):\n",
    "    print(f\"{token!r} in {category!r} → {count}\")\n",
    "\n",
    "# (Optional) Persist or save results for later stages:\n",
    "# token_cat_counts.saveAsTextFile(\"output/token_category_counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146a816-6766-4fb8-ba5a-7c4d4e691889",
   "metadata": {},
   "source": [
    "# 5. Category Document Counts with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e323a03-7494-4c52-9455-a12dc3546ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews per category:\n",
      "'Kindle_Store': 3205\n",
      "'Electronic': 7825\n",
      "'Movies_and_TV': 4607\n",
      "'Tools_and_Home_Improvement': 1926\n",
      "'Grocery_and_Gourmet_Food': 1297\n",
      "'Apps_for_Android': 2638\n",
      "'Book': 22507\n",
      "'Toys_and_Game': 2253\n",
      "'Office_Product': 1243\n",
      "'Digital_Music': 836\n",
      "'Automotive': 1374\n",
      "'Beauty': 2023\n",
      "'Patio_Lawn_and_Garde': 994\n",
      "'Sports_and_Outdoor': 3269\n",
      "'Musical_Instrument': 500\n",
      "'CDs_and_Vinyl': 3749\n",
      "'Clothing_Shoes_and_Jewelry': 5749\n",
      "'Home_and_Kitche': 4254\n",
      "'Cell_Phones_and_Accessorie': 3447\n",
      "'Pet_Supplie': 1235\n",
      "'Baby': 916\n",
      "'Health_and_Personal_Care': 2982\n"
     ]
    }
   ],
   "source": [
    "# Given `reviews: RDD[(category, reviewText)]`, count how many reviews per category:\n",
    "category_counts = (\n",
    "    reviews\n",
    "      .map(lambda cat_text: (cat_text[0], 1))       # emit (category, 1) per record\n",
    "      .reduceByKey(lambda a, b: a + b)             # sum up per category\n",
    ")\n",
    "\n",
    "# Inspect the counts\n",
    "print(\"Number of reviews per category:\")\n",
    "for category, count in category_counts.collect():\n",
    "    print(f\"{category!r}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cec1a42-5ce6-4eb8-9f29-60f237c4a7d4",
   "metadata": {},
   "source": [
    "# 6. Total Term Counts with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6fc03b96-631f-4e25-a279-e3308a17c11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample total term counts:\n",
      "'gift' → 1795\n",
      "'husband' → 1564\n",
      "'making' → 2070\n",
      "'things' → 4269\n",
      "'love' → 15659\n",
      "'directions' → 429\n",
      "'simple' → 1927\n",
      "'make' → 7180\n",
      "'raichlen' → 2\n",
      "'recipes' → 933\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from operator import add\n",
    "\n",
    "def map_tokens_to_counts(records):\n",
    "    \"\"\"\n",
    "    Local aggregation of token counts within each partition:\n",
    "      Input:  iterator over (category, tokens) pairs\n",
    "      Output: (token, count) for all tokens in that partition\n",
    "    \"\"\"\n",
    "    # accumulate in a single Counter per partition\n",
    "    partition_counter = Counter()\n",
    "    for _, tokens in records:\n",
    "        partition_counter.update(tokens)\n",
    "    # emit partial sums\n",
    "    for token, cnt in partition_counter.items():\n",
    "        yield token, cnt\n",
    "\n",
    "# 1) Local-combined counts per partition\n",
    "partial_counts = cat_tokens.mapPartitions(map_tokens_to_counts)\n",
    "\n",
    "# 2) Global aggregation like the combiner+reducer in MRTotalTermCounts\n",
    "total_term_counts = partial_counts.reduceByKey(add)\n",
    "\n",
    "# 3) Inspect a sample\n",
    "print(\"Sample total term counts:\")\n",
    "for token, count in total_term_counts.take(10):\n",
    "    print(f\"{token!r} → {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d820984-604f-4ffd-a99b-71aa61c35b57",
   "metadata": {},
   "source": [
    "# 7. Chi-Square Calculation & Top-K Terms per Category in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e07c88c-b510-464f-9ff5-1fcac6d6841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample χ² top-75 terms for a few categories:\n",
      "Kindle_Store → author:1325.7007 characters:1301.9683 series:896.3881 reading:704.1122 kindle:675.0471 romance:667.6760 short:645.0113 stories:573.5698 novella:539.4399 enjoyed:510.1735 cherri:401.2123 written:360.9085 loved:325.4744 alex:310.9019 wait:306.3657 ebook:288.4289 sex:287.0325 monash:283.1907 jae:283.0291 plot:269.1577 kade:260.3645 cernaq:259.5882 laryssa:259.5882 character:245.7995 kayla:244.4273 steamy:242.2387 forward:241.2606 mira:240.7279 persey:235.9863 love:218.9271 heroine:217.4329 garret:216.2850 leofgar:212.3849 tale:211.8060 relationship:207.5869 dani:204.0791 writing:200.5018 errors:195.8583 quality:195.0736 livie:189.3529 sophi:188.7842 sedora:188.7842 skeen:188.7842 wulfstan:188.7842 gabrielle:184.5518 bought:182.6530 aidan:181.2628 sexy:180.6496 hassel:171.6680 connor:170.1634 casey:168.0664 leopold:166.0370 sarina:165.1841 price:160.7013 naomi:160.4649 talbot:160.4649 zoe:160.2674 music:156.2964 braden:150.2092 zo:147.8478 brie:147.8478 carrie:147.7087 alpha:147.1879 woman:146.8282 osborn:142.7930 erotic:142.5051 manta:141.5846 kotrla:141.5846 marquita:141.5846 everleigh:141.5846 cyn:141.5846 tehlhaber:141.5846 jaylin:141.5846 becka:141.5846 bijou:141.5846\n",
      "Electronic → cable:6360.1151 tv:4929.6620 usb:4504.2012 computer:4139.2055 keyboard:4018.4875 sound:3571.6958 ipad:3504.0964 ipod:3446.6218 screen:3365.5259 lens:3210.3918 drive:3147.2089 device:3095.2856 speakers:3087.6433 headphones:3050.4688 battery:3033.8155 card:2976.8145 hdmi:2760.3800 gb:2611.4641 adapter:2415.9417 tablet:2360.9182 mouse:2279.0588 remote:2156.2817 works:2111.7749 wireless:2053.2616 hd:2050.0278 pc:2042.8350 player:2035.1250 router:1916.9271 cables:1884.3815 unit:1883.2409 sony:1822.4273 plug:1818.2212 windows:1670.2506 port:1574.8238 software:1466.5342 speaker:1465.3325 canon:1465.2697 video:1465.2260 quality:1324.1294 audio:1269.1411 plugged:1256.4084 apple:1252.5275 cord:1249.2117 nikon:1222.2424 receiver:1207.6217 price:1189.8804 monitor:1183.5227 signal:1151.8554 external:1141.9851 setup:1120.1818 antenna:1080.7964 lcd:1058.6651 gps:1053.0987 logitech:1045.4263 sd:1040.6278 cameras:1040.4366 mp:1013.1805 drives:986.5476 power:958.4977 system:930.8251 devices:928.9706 ports:905.3267 zoom:880.5157 flash:880.1630 radio:867.1055 mode:864.3688 tripod:855.8305 ear:851.1743 bass:846.1605 desktop:840.7433 ram:835.5388 mount:832.2767 mac:830.7911 roku:828.6208 charger:810.5654\n",
      "Movies_and_TV → dvd:14489.3163 movies:6667.1943 films:6088.6192 acting:5889.9762 season:5368.4534 actors:4168.3148 watching:4122.4538 episodes:3867.7200 watch:3750.2272 show:3598.7237 watched:3322.5647 episode:2967.2288 director:2297.6108 cast:2050.4245 comedy:2049.9605 scenes:2037.3885 actor:1952.2010 seasons:1877.5917 scene:1808.6017 blu:1658.1546 script:1425.9223 video:1357.6513 flick:1337.9122 ray:1232.1559 vhs:1223.9534 subtitles:1221.2109 funny:1181.7585 documentary:1159.3791 horror:1114.1706 role:1072.5124 action:995.5540 viewer:993.2235 dvds:981.2307 performances:957.8840 anime:946.2613 extras:922.3004 effects:873.2813 viewers:830.5369 workout:815.1684 classic:804.8146 production:783.2069 batman:781.9297 special:780.9822 cinematography:755.1168 footage:750.3919 animation:730.1869 series:724.0458 remake:705.5775 actress:686.6711 animated:682.7563 filmed:682.0403 hollywood:647.3397 cinema:641.3659 disc:619.7578 shows:589.9319 roles:576.5120 nudity:574.0340 brien:571.0490 original:564.3771 cagney:562.6298 widescreen:527.3168 played:525.6616 bruce:522.4993 aired:508.4808 plays:506.0190 character:501.1140 version:493.6830 theatrical:492.8428 performance:488.1455 cartoon:477.3430 tv:472.9810 screenplay:462.5996 cgi:458.5455 disney:455.1557 television:440.1678\n",
      "Tools_and_Home_Improvement → light:3945.9153 flashlight:3048.5324 drill:2768.6895 bulb:2492.4399 tool:2177.7215 tools:1686.1123 faucet:1648.9339 flashlights:1493.4179 bright:1330.5785 lights:1176.4352 fixture:1135.6982 dewalt:1066.4105 nailer:998.1174 thermostat:968.4204 dimmer:960.3753 batteries:897.6958 lamps:812.8808 dremel:785.1045 blade:753.2888 lathe:639.3387 cfl:638.9916 led:634.9496 shower:614.6728 miter:599.0470 blades:595.8717 switch:594.3372 ott:581.6784 makita:567.6690 lutron:559.1035 sander:519.9712 ladder:502.5783 wall:487.7573 lumens:485.7335 lumen:485.7335 beam:480.0842 ridgid:479.2194 dimmers:479.2194 hitachi:441.8432 incandescent:419.3925 dimmable:400.8459 leatherman:399.3393 showerhead:399.3393 fixtures:398.9851 bosch:397.7103 halogen:390.9302 lithium:390.4647 garage:388.5249 pliers:388.3427 door:386.2349 hose:380.8630 brighter:378.4926 sockets:375.9934 sink:361.7189 maglite:353.4566 leds:341.6901 torx:340.4618 watt:324.4695 kohler:321.6671 kidde:319.4634 installed:315.9797 sanding:311.5869 screw:304.5208 mortise:279.5269 paslode:279.5269 nailers:279.5269 culligan:279.5269 feit:279.5269 milwaukee:276.5476 chandelier:276.5476 volt:275.1440 wrench:270.8539 ceiling:269.2924 strobe:265.7718 ld:265.7718 fence:262.7033\n",
      "Grocery_and_Gourmet_Food → tea:5784.8175 chocolate:3281.0646 flavors:2597.5007 snack:2551.3965 sugar:2254.5060 tastes:1950.6297 delicious:1911.5724 tasting:1697.9751 flavored:1631.6574 sauce:1507.1580 organic:1393.5266 butter:1390.5928 gluten:1358.3216 teas:1288.0698 cocoa:1280.4326 stevia:1233.3246 peanut:1191.6459 drink:1190.0372 coconut:1130.9969 tasted:1086.4994 crackers:1077.2602 spicy:1077.2602 sweetener:1034.7076 corn:999.6363 cereal:980.4619 cookies:958.2166 creamy:951.7661 salt:899.5153 soup:891.0005 salty:888.9385 texture:882.8177 milk:870.2434 cinnamon:858.6220 cup:853.3899 coffees:842.8798 tasty:806.9360 bitter:788.2640 tofu:787.8232 rice:785.2361 seasoning:759.7072 candy:718.6538 flavorful:709.2062 grocery:691.2809 almonds:684.4199 chewy:679.1601 chia:665.1916 keurig:654.7658 granola:652.9531 crunchy:636.0965 eat:635.7821 roast:635.4492 candies:632.1037 flour:624.9995 hazelnut:611.5437 chai:611.5437 oatmeal:608.2404 ginger:599.6943 vanilla:584.6699 seeds:581.4206 sodium:553.0720 darjeeling:541.6948 tazo:538.0630 matcha:538.0630 pasta:537.1566 breakfast:532.3131 chips:531.0732 calories:523.1788 chocolates:522.5799 starbucks:520.3898 oil:514.1001 flax:512.0397 honey:502.0714 beans:495.6230 ghee:494.8972 cups:492.8791\n"
     ]
    }
   ],
   "source": [
    "def compute_chi2_top_terms(token_cat_counts_rdd, \n",
    "                           category_counts_rdd, \n",
    "                           total_term_counts_rdd, \n",
    "                           top_k=75):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      token_cat_counts_rdd: RDD[((token, category), A)]                # A = total times token appears in category\n",
    "      category_counts_rdd:   RDD[(category, N_i)]                      # N_i = number of reviews in category\n",
    "      total_term_counts_rdd: RDD[(token, n_j)]                         # n_j = total times token appears overall\n",
    "    Returns:\n",
    "      RDD[(category, \"term1:chi2 term2:chi2 ...\")] with top_k terms per category.\n",
    "    \"\"\"\n",
    "    # 1) collect & broadcast the small side-tables\n",
    "    category_reviews = category_counts_rdd.collectAsMap()  # {category: N_i}\n",
    "    N = sum(category_reviews.values())                     # total number of reviews\n",
    "    bc_cat_rev = sc.broadcast(category_reviews)\n",
    "    \n",
    "    term_totals = total_term_counts_rdd.collectAsMap()     # {term: n_j}\n",
    "    bc_term_tot = sc.broadcast(term_totals)\n",
    "    \n",
    "    # 2) compute χ² for each ((term, category), A)\n",
    "    def compute_chi2(kv):\n",
    "        (term, category), A = kv\n",
    "        N_i = bc_cat_rev.value.get(category, 0)\n",
    "        n_j = bc_term_tot.value.get(term, 0)\n",
    "        B = N_i - A\n",
    "        C = n_j - A\n",
    "        D = N - A - B - C\n",
    "        num = (A * D - B * C) ** 2\n",
    "        den = (A + B) * (C + D) * (A + C) * (B + D)\n",
    "        chi2 = (N * num / den) if den != 0 else 0.0\n",
    "        return category, (term, chi2)\n",
    "    \n",
    "    chi2_rdd = token_cat_counts_rdd.map(compute_chi2)\n",
    "    \n",
    "    # 3) group by category and pick top_k by χ²\n",
    "    top_rdd = (\n",
    "        chi2_rdd\n",
    "          .groupByKey()\n",
    "          .mapValues(lambda seq: sorted(seq, key=lambda x: -x[1])[:top_k])\n",
    "          .mapValues(lambda top: \" \".join(f\"{t}:{c:.4f}\" for t, c in top))\n",
    "    )\n",
    "    \n",
    "    return top_rdd\n",
    "\n",
    "# ───────── Usage ─────────\n",
    "\n",
    "chi2_top75 = compute_chi2_top_terms(\n",
    "    token_cat_counts,   # from step 4\n",
    "    category_counts,    # from step 5\n",
    "    total_term_counts,  # from step 7\n",
    "    top_k=75\n",
    ")\n",
    "\n",
    "print(\"Sample χ² top-75 terms for a few categories:\")\n",
    "for category, term_list in chi2_top75.take(5):\n",
    "    print(f\"{category} → {term_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0c441-acc0-4d31-ac8d-8ead168cf1e3",
   "metadata": {},
   "source": [
    "# 8. Merge chi-squared outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3cda328b-abfd-4a83-90bd-f9188ced8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `chi2_top75` is an RDD[(category, term_list_str)]\n",
    "# 1) Save the RDD as a single text file locally:\n",
    "lines = chi2_top75.map(lambda kv: f\"{json.dumps(kv[0])}\\t{kv[1]}\")\n",
    "lines.coalesce(1).saveAsTextFile(\"output/merged_chi2_output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778ca8e-d8ea-45eb-b4e6-a2e182f42961",
   "metadata": {},
   "source": [
    "# 9. Final Formatting in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "424f1031-8fb4-4c54-9ed1-9a87dfb64489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote final formatted output to output/output.txt\n"
     ]
    }
   ],
   "source": [
    "# 1) Collect and sort the (category, term:χ²…) lines\n",
    "cat_lines = chi2_top75.collect()  # e.g. [(\"Books\", \"author:123.4 ...\"), ...]\n",
    "cat_lines_sorted = sorted(cat_lines, key=lambda kv: kv[0])\n",
    "\n",
    "# 2) Compute the set of all unique tokens\n",
    "unique_terms = {\n",
    "    term_score.split(\":\", 1)[0]\n",
    "    for _, term_list in cat_lines_sorted\n",
    "    for term_score in term_list.split()\n",
    "}\n",
    "unique_terms_sorted = sorted(unique_terms)\n",
    "\n",
    "# 3) Write out exactly the same format as final_output.py\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "with open(\"output/output.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    # one category per line\n",
    "    for category, term_list in cat_lines_sorted:\n",
    "        fout.write(f\"{category} {term_list}\\n\")\n",
    "    # final line: all unique terms sorted alphabetically\n",
    "    fout.write(\" \".join(unique_terms_sorted) + \"\\n\")\n",
    "\n",
    "print(\"Wrote final formatted output to output/output.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d84a1-0fb1-430c-83bb-7641f9eef021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
