{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e7819f-4323-4d2f-8b1f-2cdf2f1c94a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 19:25:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover, CountVectorizer,\n",
    "    IDF, ChiSqSelector\n",
    ")\n",
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "#initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmazonReviewsTFIDF\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e671a36-08aa-4954-96a7-949e465880e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "df = spark.read.json(input_path)\n",
    "\n",
    "#lowercase to make fewer instances\n",
    "df = df.withColumn(\"reviewTextLower\", lower(col(\"reviewText\")))\n",
    "df = df.withColumn(\"label\", col(\"overall\"))  # Or use a constant like lit(0.0) if labels are not yet meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b42b38-219c-426e-ae2f-a2c9984fe102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, category: string, helpful: array<bigint>, overall: double, reviewText: string, reviewTime: string, reviewerID: string, reviewerName: string, summary: string, unixReviewTime: bigint, reviewTextLower: string, label: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069355d-13a5-4edb-8837-b18f699a1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization step\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"reviewTextLower\",\n",
    "    outputCol=\"tokens\",\n",
    "    pattern = r\"\"\"[\\s\\d()\\[\\]{}.!?,;:+=\\-_\"'`~#@&*%€$§\\\\/]+\"\"\",\n",
    "    toLowercase=True\n",
    ")\n",
    "\n",
    "#remove stopwords\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"tokens\",\n",
    "    outputCol=\"filtered_tokens\"\n",
    ")\n",
    "\n",
    "#vectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    inputCol=\"filtered_tokens\",\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "#TF-IDF\n",
    "idf = IDF(\n",
    "    inputCol=\"raw_features\",\n",
    "    outputCol=\"features\",\n",
    "    minDocFreq=5\n",
    ")\n",
    "\n",
    "#Chi-Square\n",
    "selector = ChiSqSelector(\n",
    "    numTopFeatures=2000,\n",
    "    featuresCol=\"features\",\n",
    "    outputCol=\"selected_features\",\n",
    "    labelCol=\"label\"\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, vectorizer, idf, selector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac472f-9328-480b-bbcc-545ff26ff9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df)\n",
    "\n",
    "#extracting vocabulary and top terms\n",
    "cv_model = model.stages[2] \n",
    "selector_model = model.stages[4]  \n",
    "\n",
    "vocab = cv_model.vocabulary\n",
    "top_indices = selector_model.selectedFeatures\n",
    "\n",
    "top_terms = [vocab[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f2767-d007-4e66-aab5-d5679a9bc597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2000 terms saved to output_ds.txt\n"
     ]
    }
   ],
   "source": [
    "#top 2000 terms to output file\n",
    "output_path = \"output_ds.txt\"  \n",
    "with open(output_path, \"w\") as f:\n",
    "    for term in top_terms:\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"Top 2000 terms saved to {output_path}\")\n",
    "\n",
    "#stop the Spark session - not now, only at the end of Part 3\n",
    "#spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51a725ba-ba6e-4b3b-99fc-be29babd8aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d1fd8",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b0dd95",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c10cb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|5.0  |46957|\n",
      "|4.0  |15239|\n",
      "|3.0  |6644 |\n",
      "|1.0  |6095 |\n",
      "|2.0  |3894 |\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I got the error in grid search, that at a point I have a split that only one class is present - so let's check how the class samples are originally and after splitting\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "#count reviews per category (i.e., label)\n",
    "df.groupBy(\"label\").agg(count(\"*\").alias(\"count\")).orderBy(\"count\", ascending=False).show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb914673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "#indexing for OneVsRest\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(df)\n",
    "\n",
    "df = indexer.transform(df)\n",
    "selector.setParams(labelCol=\"indexedLabel\")\n",
    "\n",
    "#split the data into training (60%), validation (20%), and test (20%) sets according to the exercise description\n",
    "train_val, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "train, val = train_val.randomSplit([0.75, 0.25], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead3cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution:\n",
      "+------------+-----+\n",
      "|indexedLabel|count|\n",
      "+------------+-----+\n",
      "|0.0         |28299|\n",
      "|1.0         |9188 |\n",
      "|2.0         |4027 |\n",
      "|3.0         |3650 |\n",
      "|4.0         |2326 |\n",
      "+------------+-----+\n",
      "\n",
      "Validation set class distribution:\n",
      "+------------+-----+\n",
      "|indexedLabel|count|\n",
      "+------------+-----+\n",
      "|0.0         |9354 |\n",
      "|1.0         |2982 |\n",
      "|2.0         |1328 |\n",
      "|3.0         |1232 |\n",
      "|4.0         |779  |\n",
      "+------------+-----+\n",
      "\n",
      "Test set class distribution:\n",
      "+------------+-----+\n",
      "|indexedLabel|count|\n",
      "+------------+-----+\n",
      "|0.0         |9304 |\n",
      "|1.0         |3069 |\n",
      "|2.0         |1289 |\n",
      "|3.0         |1213 |\n",
      "|4.0         |789  |\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training set\n",
    "print(\"Training set class distribution:\")\n",
    "train.groupBy(\"indexedLabel\").agg(count(\"*\").alias(\"count\")).orderBy(\"count\", ascending=False).show(50, truncate=False)\n",
    "\n",
    "#validation set\n",
    "print(\"Validation set class distribution:\")\n",
    "val.groupBy(\"indexedLabel\").agg(count(\"*\").alias(\"count\")).orderBy(\"count\", ascending=False).show(50, truncate=False)\n",
    "\n",
    "#test set\n",
    "print(\"Test set class distribution:\")\n",
    "test.groupBy(\"indexedLabel\").agg(count(\"*\").alias(\"count\")).orderBy(\"count\", ascending=False).show(50, truncate=False)\n",
    "\n",
    "#sample sizes are enough, so we might have a problem with OneVsRest identifying the classes -> indexing might help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9600b",
   "metadata": {},
   "source": [
    "## Classification pipeline and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearSVC - SVM classifier with linear kernel\n",
    "#OneVsRest - to handle multiclass classification by binary classifier\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "\n",
    "#to estimate performance using F1 as criterion\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#vector length normalization L2\n",
    "from pyspark.ml.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33088244-472d-48b0-91e4-9192af223a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15675"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for make the code more efficient, we preprocess with TF-IDF once and cache\n",
    "vectorizer.setParams(outputCol=\"raw_features\", vocabSize=5000)\n",
    "idf.setParams(inputCol=\"raw_features\", outputCol=\"features\", minDocFreq=5)\n",
    "\n",
    "pre_pipeline = Pipeline(stages=[tokenizer, remover, vectorizer, idf])\n",
    "pre_model = pre_pipeline.fit(train)\n",
    "\n",
    "pre_train = pre_model.transform(train).cache()\n",
    "pre_val = pre_model.transform(val).cache()\n",
    "pre_train.count()  #trigger caching\n",
    "pre_val.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b974a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizer for L2 norm\n",
    "normalizer = Normalizer(inputCol=\"selected_features\", outputCol=\"norm_features\", p=2.0)\n",
    "\n",
    "#evaluator for F1 score\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e5ccf",
   "metadata": {},
   "source": [
    "## Grid search for parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed32d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search with 24 configurations...\\n\n",
      "[1/24] std=True, features=2000, reg=0.1, iter=10\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.5613 (took 37.3s)\\n\n",
      "[2/24] std=True, features=2000, reg=0.1, iter=50\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.5449 (took 60.6s)\\n\n",
      "[3/24] std=True, features=2000, reg=0.01, iter=10\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.5901 (took 24.1s)\\n\n",
      "[4/24] std=True, features=2000, reg=0.01, iter=50\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.5651 (took 56.5s)\\n\n",
      "[5/24] std=True, features=2000, reg=0.001, iter=10\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.5918 (took 22.8s)\\n\n",
      "[6/24] std=True, features=2000, reg=0.001, iter=50\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.5686 (took 53.8s)\\n\n",
      "[7/24] std=True, features=500, reg=0.1, iter=10\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.5359 (took 21.3s)\\n\n",
      "[8/24] std=True, features=500, reg=0.1, iter=50\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.5315 (took 57.9s)\\n\n",
      "[9/24] std=True, features=500, reg=0.01, iter=10\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.5552 (took 20.6s)\\n\n",
      "[10/24] std=True, features=500, reg=0.01, iter=50\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.5487 (took 56.2s)\\n\n",
      "[11/24] std=True, features=500, reg=0.001, iter=10\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.5558 (took 21.1s)\\n\n",
      "[12/24] std=True, features=500, reg=0.001, iter=50\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.5555 (took 59.0s)\\n\n",
      "[13/24] std=False, features=2000, reg=0.1, iter=10\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.4460 (took 33.2s)\\n\n",
      "[14/24] std=False, features=2000, reg=0.1, iter=50\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.4460 (took 66.2s)\\n\n",
      "[15/24] std=False, features=2000, reg=0.01, iter=10\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.4460 (took 23.6s)\\n\n",
      "[16/24] std=False, features=2000, reg=0.01, iter=50\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.4460 (took 54.6s)\\n\n",
      "[17/24] std=False, features=2000, reg=0.001, iter=10\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.4667 (took 21.6s)\\n\n",
      "[18/24] std=False, features=2000, reg=0.001, iter=50\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.4578 (took 52.6s)\\n\n",
      "[19/24] std=False, features=500, reg=0.1, iter=10\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.4460 (took 26.0s)\\n\n",
      "[20/24] std=False, features=500, reg=0.1, iter=50\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.4460 (took 60.4s)\\n\n",
      "[21/24] std=False, features=500, reg=0.01, iter=10\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.4460 (took 21.6s)\\n\n",
      "[22/24] std=False, features=500, reg=0.01, iter=50\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.4460 (took 55.4s)\\n\n",
      "[23/24] std=False, features=500, reg=0.001, iter=10\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.4901 (took 20.8s)\\n\n",
      "[24/24] std=False, features=500, reg=0.001, iter=50\n",
      "fitting model\n",
      "predictions\n",
      "evaluation\n",
      "appending results\n",
      " → F1 score: 0.4845 (took 56.3s)\\n\n",
      "Total grid search duration: 16.39 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "total_runs = 2 * 2 * 3 * 2  # standardization × num_features × reg_param × max_iter\n",
    "run_num = 1\n",
    "\n",
    "\"\"\"\n",
    "Possible parameters for comparison (according to the exercise description):\n",
    "- standardization of training features (2 values): True or False\n",
    "- number of features: 2000 (given in the exercise), 500 (much havier filtering)\n",
    "- regularization parameter (3 values): 0.1, 0.01 or 0.001\n",
    "- maximum number of iterations (2 values): 10 or 50\n",
    "\"\"\"\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "print(f\"Starting grid search with {total_runs} configurations...\\\\n\")\n",
    "\n",
    "#iterating through the possible parameter combinations\n",
    "for standardize in [True, False]:\n",
    "    for num_features in [2000, 500]:\n",
    "        for reg_param in [0.1, 0.01, 0.001]:\n",
    "            for max_iter in [10, 50]:\n",
    "                run_start = time.time()\n",
    "                print(f\"[{run_num}/{total_runs}] std={standardize}, features={num_features}, reg={reg_param}, iter={max_iter}\")\n",
    "\n",
    "\n",
    "                try: \n",
    "                    selector = ChiSqSelector(featuresCol=\"features\", outputCol=\"selected_features\", labelCol=\"indexedLabel\", numTopFeatures=num_features)\n",
    "                    normalizer = Normalizer(inputCol=\"selected_features\", outputCol=\"norm_features\", p=2.0)\n",
    "                    \n",
    "                    #create the Linear SVM model with the current parameters and using the one vs all strategy\n",
    "                    svm = LinearSVC(featuresCol=\"norm_features\", labelCol=\"indexedLabel\", regParam=reg_param, maxIter=max_iter, standardization=standardize)\n",
    "                    ovr = OneVsRest(classifier=svm, featuresCol=\"norm_features\", labelCol=\"indexedLabel\")\n",
    "    \n",
    "                    #adding the normalizer and the classifier to create full pipeline\n",
    "                    pipeline = Pipeline(stages=[selector, normalizer, ovr])\n",
    "                    \n",
    "                    #train on train set\n",
    "                    print(\"fitting model\")\n",
    "                    model = pipeline.fit(pre_train)\n",
    "\n",
    "                    #evaluate on val set\n",
    "                    print(\"predictions\")\n",
    "                    predictions = model.transform(pre_val)\n",
    "                    print(\"evaluation\")\n",
    "                    f1 = evaluator.evaluate(predictions)\n",
    "\n",
    "                    #save results\n",
    "                    print(\"appending results\")\n",
    "                    results.append({\n",
    "                        \"standardization\": standardize,\n",
    "                        \"num_features\": num_features,\n",
    "                        \"reg_param\": reg_param,\n",
    "                        \"max_iter\": max_iter,\n",
    "                        \"f1_score\": f1\n",
    "                    })\n",
    "\n",
    "                    duration = time.time() - run_start\n",
    "                    print(f\" → F1 score: {f1:.4f} (took {duration:.1f}s)\\\\n\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"!! Skipping due to error: {e}\")\n",
    "\n",
    "                run_num += 1\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"Total grid search duration: {total_time/60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5818e7",
   "metadata": {},
   "source": [
    "## Grid search results - best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2001a58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Results:\n",
      "    standardization  num_features  reg_param  max_iter  f1_score\n",
      "4              True          2000      0.001        10  0.591755\n",
      "2              True          2000      0.010        10  0.590108\n",
      "5              True          2000      0.001        50  0.568629\n",
      "3              True          2000      0.010        50  0.565129\n",
      "0              True          2000      0.100        10  0.561325\n",
      "10             True           500      0.001        10  0.555844\n",
      "11             True           500      0.001        50  0.555489\n",
      "8              True           500      0.010        10  0.555182\n",
      "9              True           500      0.010        50  0.548708\n",
      "1              True          2000      0.100        50  0.544945\n",
      "6              True           500      0.100        10  0.535852\n",
      "7              True           500      0.100        50  0.531492\n",
      "22            False           500      0.001        10  0.490120\n",
      "23            False           500      0.001        50  0.484537\n",
      "16            False          2000      0.001        10  0.466713\n",
      "17            False          2000      0.001        50  0.457810\n",
      "15            False          2000      0.010        50  0.446040\n",
      "14            False          2000      0.010        10  0.446040\n",
      "13            False          2000      0.100        50  0.446040\n",
      "12            False          2000      0.100        10  0.446040\n",
      "19            False           500      0.100        50  0.446040\n",
      "18            False           500      0.100        10  0.446040\n",
      "21            False           500      0.010        50  0.446040\n",
      "20            False           500      0.010        10  0.446040\n",
      "Grid search results saved to grid_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"f1_score\", ascending=False)\n",
    "\n",
    "print(\"Grid Search Results:\")\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_csv(\"grid_search_results.csv\", index=False)\n",
    "print(\"Grid search results saved to grid_search_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e11c87",
   "metadata": {},
   "source": [
    "## Evaluate best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57532e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration F1 on test set: 0.5792334853581884\n",
      "Final model info saved to final_model_results.txt\n"
     ]
    }
   ],
   "source": [
    "#best model on best parameters\n",
    "best_params = results_df.iloc[0]\n",
    "selector.setNumTopFeatures(best_params[\"num_features\"])\n",
    "\n",
    "svm = LinearSVC(featuresCol = \"norm_features\", labelCol = \"indexedLabel\", regParam = best_params[\"reg_param\"], maxIter = int(best_params[\"max_iter\"]), standardization = bool(best_params[\"standardization\"]))\n",
    "ovr = OneVsRest(classifier = svm, featuresCol = \"norm_features\", labelCol = \"indexedLabel\")\n",
    "\n",
    "final_pipeline = Pipeline(stages = [tokenizer, remover, vectorizer, idf, selector, normalizer, ovr])\n",
    "final_model = final_pipeline.fit(train)\n",
    "\n",
    "#predict and evaluate on test set\n",
    "test_predictions = final_model.transform(test)\n",
    "f1_test = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Best configuration F1 on test set:\", f1_test)\n",
    "\n",
    "with open(\"final_model_results.txt\", \"w\") as f:\n",
    "    f.write(\"Best Configuration:\\\\n\")\n",
    "    f.write(str(best_params.to_dict()) + \"\\\\n\")\n",
    "    f.write(f\"F1 Score on Test Set: {f1_test:.4f}\\\\n\")\n",
    "\n",
    "print(\"Final model info saved to final_model_results.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9989a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc48f2-c98b-46cb-be41-f4034657ad46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
